# Python program to generate WordCloud, make sure WordCloud, Matplotlib, and Pandas are installed 

#import streamlit, io, pandas, matplotlib, wordcloud
import streamlit as st
from io import StringIO
from io import BytesIO
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS
import streamlit_ext as ste 
from unidecode import unidecode

##-------Funtions to be excuted in the application-------
##Convert txt input file to bytes format for app
def convert_stopwordfile (stopword_file):
    bytes_data = stopword_file.read()
    sw_list=bytes_data.decode("utf-8").splitlines()
    return sw_list

##Convert plot from bytes format to downloadable png format
def convert_plot():
    buffer = BytesIO()
    dataset = plt.savefig(buffer,format="png")
    buffer.seek(0)
    image_png = buffer.getvalue()
    buffer.close()
    return image_png

#Convert dataframe to txt for export
def convert_df(df):
    return df.to_csv(index=False, header=False).encode('utf-8')

##-------StopWords Template Setup-------
template_stopwords = ["a",
"a's",
"able",
"about",
"above",
"according",
"accordingly",
"across",
"actually",
"after",
"afterwards",
"again",
"against",
"ain't",
"all",
"allow",
"allows",
"almost",
"alone",
"along",
"already",
"also",
"although",
"always",
"am",
"among",
"amongst",
"an",
"and",
"another",
"any",
"anybody",
"anyhow",
"anyone",
"anything",
"anyway",
"anyways",
"anywhere",
"apart",
"appear",
"are",
"aren't",
"around",
"as",
"aside",
"ask",
"asking",
"associated",
"at",
"available",
"away",
"b",
"be",
"became",
"because",
"become",
"becomes",
"becoming",
"been",
"before",
"beforehand",
"behind",
"being",
"believe",
"below",
"beside",
"besides",
"between",
"beyond",
"both",
"brief",
"but",
"by",
"c",
"c'mon",
"c's",
"came",
"can",
"can't",
"cannot",
"cant",
"cause",
"causes",
"certain",
"certainly",
"changes",
"clearly",
"co",
"com",
"come",
"comes",
"consequently",
"contain",
"containing",
"contains",
"corresponding",
"could",
"couldn't",
"course",
"currently",
"d",
"definitely",
"described",
"despite",
"did",
"didn't",
"different",
"do",
"does",
"doesn't",
"doing",
"don't",
"done",
"down",
"downwards",
"during",
"e",
"each",
"edu",
"eg",
"eight",
"either",
"else",
"elsewhere",
"enough",
"entirely",
"especially",
"et",
"etc",
"even",
"ever",
"every",
"everybody",
"everyone",
"everything",
"everywhere",
"ex",
"exactly",
"example",
"except",
"f",
"far",
"few",
"fifth",
"first",
"five",
"followed",
"following",
"follows",
"for",
"former",
"formerly",
"forth",
"four",
"from",
"further",
"furthermore",
"g",
"get",
"gets",
"getting",
"given",
"gives",
"go",
"goes",
"going",
"gone",
"got",
"gotten",
"greetings",
"h",
"had",
"hadn't",
"happens",
"hardly",
"has",
"hasn't",
"have",
"haven't",
"having",
"he",
"he's",
"hello",
"help",
"hence",
"her",
"here",
"here's",
"hereafter",
"hereby",
"herein",
"hereupon",
"hers",
"herself",
"hi",
"him",
"himself",
"his",
"hither",
"how",
"howbeit",
"however",
"i",
"i'd",
"i'll",
"i'm",
"i've",
"ie",
"if",
"ignored",
"immediate",
"in",
"inasmuch",
"inc",
"indeed",
"indicate",
"indicated",
"indicates",
"inner",
"insofar",
"instead",
"into",
"inward",
"is",
"isn't",
"it",
"it'd",
"it'll",
"it's",
"its",
"itself",
"j",
"just",
"k",
"keep",
"keeps",
"kept",
"know",
"knows",
"known",
"l",
"last",
"lately",
"later",
"latter",
"latterly",
"lest",
"let",
"let's",
"like",
"liked",
"likely",
"little",
"look",
"looking",
"looks",
"ltd",
"m",
"mainly",
"many",
"may",
"maybe",
"me",
"mean",
"meanwhile",
"merely",
"might",
"more",
"moreover",
"most",
"mostly",
"much",
"must",
"my",
"myself",
"n",
"name",
"namely",
"nd",
"near",
"nearly",
"needs",
"neither",
"never",
"nevertheless",
"new",
"next",
"nine",
"no",
"nobody",
"non",
"none",
"noone",
"nor",
"normally",
"not",
"nothing",
"novel",
"now",
"nowhere",
"o",
"obviously",
"of",
"off",
"often",
"oh",
"ok",
"okay",
"old",
"on",
"once",
"one",
"ones",
"only",
"onto",
"or",
"other",
"others",
"otherwise",
"ought",
"our",
"ours",
"ourselves",
"out",
"outside",
"over",
"overall",
"own",
"p",
"particular",
"particularly",
"per",
"perhaps",
"placed",
"please",
"plus",
"possible",
"presumably",
"probably",
"provides",
"q",
"que",
"quite",
"qv",
"r",
"rather",
"rd",
"re",
"really",
"reasonably",
"regarding",
"regardless",
"regards",
"relatively",
"respectively",
"right",
"s",
"said",
"same",
"saw",
"say",
"saying",
"says",
"second",
"secondly",
"see",
"seeing",
"seem",
"seemed",
"seeming",
"seems",
"seen",
"self",
"selves",
"sensible",
"sent",
"serious",
"seriously",
"seven",
"several",
"shall",
"she",
"should",
"shouldn't",
"since",
"six",
"so",
"some",
"somebody",
"somehow",
"someone",
"something",
"sometime",
"sometimes",
"somewhat",
"somewhere",
"soon",
"sorry",
"specified",
"specify",
"specifying",
"still",
"sub",
"such",
"sup",
"sure",
"t",
"t's",
"take",
"taken",
"tell",
"tends",
"th",
"than",
"thank",
"thanks",
"thanx",
"that",
"that's",
"thats",
"the",
"their",
"theirs",
"them",
"themselves",
"then",
"thence",
"there",
"there's",
"thereafter",
"thereby",
"therefore",
"therein",
"theres",
"thereupon",
"these",
"they",
"they'd",
"they'll",
"they're",
"they've",
"think",
"third",
"this",
"thorough",
"thoroughly",
"those",
"though",
"three",
"through",
"throughout",
"thru",
"thus",
"to",
"together",
"too",
"took",
"toward",
"towards",
"tried",
"tries",
"truly",
"try",
"trying",
"twice",
"two",
"u",
"un",
"under",
"unless",
"unlikely",
"until",
"unto",
"up",
"upon",
"us",
"use",
"used",
"useful",
"uses",
"using",
"usually",
"uucp",
"v",
"value",
"various",
"very",
"via",
"viz",
"vs",
"w",
"want",
"wants",
"was",
"wasn't",
"way",
"we",
"we'd",
"we'll",
"we're",
"we've",
"welcome",
"well",
"went",
"were",
"weren't",
"what",
"what's",
"whatever",
"when",
"whence",
"whenever",
"where",
"where's",
"whereafter",
"whereas",
"whereby",
"wherein",
"whereupon",
"wherever",
"whether",
"which",
"while",
"whither",
"who",
"who's",
"whoever",
"whole",
"whom",
"whose",
"why",
"will",
"willing",
"wish",
"with",
"within",
"without",
"won't",
"wonder",
"would",
"would",
"wouldn't",
"x",
"y",
"yes",
"yet",
"you",
"you'd",
"you'll",
"you're",
"you've",
"your",
"yours",
"yourself",
"yourselves",
"z",
"zero",
"treated",
"treatment",
"trial",
"clinical",
"versus",
"phase",
"study",
"therapy",
"efficacy",
"safety",
"analysis",
"review",
"multiple",
"myeloma",
"MM",
"RRMM",
"relapsed",
"refractory",
"patients",
"disease",
"review",
"results",
"cells",
"cell",
"Background",
"Methods",
"Results",
"Conclusions",
"including",
"month",
"day",
"pt",
"received",
"data",
"pts",
"months",
"days"]
df_stopwords_template = pd.DataFrame(template_stopwords,columns=[''])
#st.write(df_stopwords_template) #df works for full stop list of template words
#stop_template = with open

##-------App Title-------
# Add Title for Streamlit App
st.title('Word Cloud Tool')

##-------Work Instructions Tool Overview and Process Steps-------
st.markdown("The word cloud tool creates an image showing the frequency of word use with smaller words used less frequently and larger words used more frequently. A Stopword list is used to filter out words that you want to ignore. Follow these instructions to create your word cloud.")
st.markdown("""
1. Prepare your excel spreadsheet by labeling the first cell of the column which includes your source data as CONTENT (**ensure all caps**)
2. Save your file as a CSV
3. Prepare your stopwords txt file by downloading the txt file, adding or removing any words to be ignored and save
4. Add your spreadsheet and txt files to the tool
5. Select any parameters you would like to apply
6. Examine the output â€“ if you want to remove additional words, add to your stopword file, save and reupload your new stopwords
7. Download your Word Cloud!
""")

##-------File Uploader Widgets-------
# File Downloader Widget for StopWord List Template
stop_word_template = convert_df(df_stopwords_template)
st.subheader('Download Template StopWord List (Optional)')
ste.download_button(
    label="Download StopWord Template.txt",
    data=stop_word_template,##text file - reformat text file conversion from df
    file_name='StopWordTemplate.txt', ##text file name assigned
    mime='text/csv', ##data type
    )
# File Uploader widgets with specified .csv and .txt file types with error statements returned
st.subheader("Select a CSV file")
content_uploaded_file = st.file_uploader("Choose a CSV file", type = [".csv"])
st.subheader("Select a stopword text file")
stopword_uploaded_file = st.file_uploader("Choose a text file", type = [".txt"])


##-------Body of App-------
if content_uploaded_file and stopword_uploaded_file is not None:
    #add check point to see if files are uploaded
    df = pd.read_csv(content_uploaded_file,encoding="latin-1") #error until files loaded
    df = df["CONTENT"].apply(unidecode)
    #input file take as bytes data, need to read out information from bytes back to string - need to append to list after
    st.subheader('Review Input data (Optional)')
    preview = st.checkbox('Show input data')
    preview_placeholder = st.empty()
    if preview:
        with preview_placeholder.container():
            st.subheader('Input data')
            st.write(df)
            #st.write("Review your input data before continuing to run the tool")
            st.stop() #Box checked: stops run
    else:
        preview_placeholder.empty() #Box unchecked: continues to run and no dataframe shown


    comment_words = ""

    placeholder=st.empty()
    placeholder.text("Generating WordCloud. Please wait...")

    for val in df: 
    # typecaste each val to string 
        val = str(val) 
    # split the value 
        tokens = val.split()     
        # Converts each token into lowercase 
        for i in range(len(tokens)): 
            tokens[i] = tokens[i].lower()  
        for words in tokens:
            comment_words = comment_words + words + ' '
    #st.write(comment_words)
    placeholder.empty()
    
    st.subheader('Customize WordCloud Design(Optional)')
    wordcloud = WordCloud(width = 800, height = 800, 
        background_color = st.color_picker('Select a background color','#fff'), 
        stopwords = convert_stopwordfile(stopword_uploaded_file),
        max_words = st.slider('Select the number of words to be displayed',1,50,25),
        min_font_size = 10).generate(comment_words)

# plot the WordCloud image
    figure = plt.figure(figsize = (8, 8), facecolor = None)
    # placeholder.empty() 
    plt.imshow(wordcloud) 
    plt.axis("off") 
    plt.tight_layout(pad = 0)
    st.pyplot(figure)
    plot_data = convert_plot()
    #use streamlit extension to download png without rerunning code
    st.subheader('Download WordCloud Image')
    ste.download_button(
        label="Download WordCloud.png",
        data=plot_data,
        file_name='WordCloud.png',
        mime='image/png',
    )

